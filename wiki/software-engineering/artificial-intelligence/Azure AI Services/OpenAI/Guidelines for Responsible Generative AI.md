---
tags: azure, azureai, azureopenai, azuregenerativeai
---

# Guidline to identify, test and mitigate harms

With such powerful capabilities, generative AI brings with it some dangers; and requires that data scientists, developers, and others involved in creating generative AI solutions adopt a responsible approach that identifies, measures, and mitigates risks.

The Microsoft guidance for responsible generative AI is designed to be practical and actionable. It defines a 3 stage process to develop and implement a plan for responsible AI when using generative models:

1. **Identify** potential harms that are relevant to your planned solution.

    1. Potential harms depend on multiple factors, including the specific services and models used to generate output as well as any fine-tuning or custom data
    2. Prepare a diverse selection of input prompts that are likely to result in potential harm
    3. Prioritise identified harms and focus on high priority harms
    4. Document harms and share with Stakeholders etc.

2. **Test** the presence of these harms in the outputs generated by your solution.

    1. Submit the prompts to the system and retrieve the generated output
    2. Apply pre-defined criteria to evaluate the output and categorise it according to the level of potential harm it contains
    3. When automate testing, always manual test from time to time

3. **Mitigate** the harms at multiple layers in your solution to minimise their presence and impact, and ensure transparent communication about potential risks to users
    1. Model layer - Select a model that is appropriate to use-case. For small text input, it's not required to use ChatGPT 4 model which can lead to more harms
    2. Safety layer - Azure OpenAI Service supports content filters etc.
    3. User experience layer - Think about the user interface and how you can mitigate the risk of harmful responses like adding input validation etc.

Repeat steps.

# Release Reponsible AI

Before releasing a generative AI solution, identify the various compliance requirements in your organization and industry and ensure the appropriate teams are given the opportunity to review the system and its documentation. Common compliance reviews include:

-   Legal
-   Privacy
-   Security
-   Accessibility

A successful release requires some planning and preparation. Consider the following guidelines:

-   Devise a *phased delivery plan* that enables you to release the solution initially to restricted group of users
-   Create an *incident response plan* that includes estimates of the time taken to respond to unanticipated incidents.
-   Create a *rollback plan* that defines the steps to revert the solution to a previous state in the event of an incident.
-   Implement the capability to immediately block harmful system responses when they're discovered.
-   Implement a capability to block specific users, applications, or client IP addresses in the event of system misuse.
-   Implement a way for users to provide feedback and report issues. In particular, enable users to report generated content as "inaccurate", "incomplete", "harmful", "offensive", or otherwise problematic.
-   Track telemetry data that enables you to determine user satisfaction and identify functional gaps or usability challenges. Telemetry collected should comply with privacy laws and your own organization's policies and commitments to user privacy.

# Content Filters

Azure OpenAI Services already comes with default content filters. In Azure OpenAI Studio you can also create custom content filters for the 4 cateogries:

1. Hate
2. Sexual
3. Self-harm
4. Violence

In order make the filter less restrictive (allow more) a permission from Microsoft is necessary.
